# -*- coding: utf-8 -*-
"""Titanic Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rcZ7LkQhsbVeUbTCVanTn3KNyGua6rBz
"""

#Data Preprocessing
import numpy as np
import pandas.util.testing as tm
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
uploaded = files.upload()
df = pd.read_csv('train.csv')

df.shape

df.dtypes

df.shape
df.head(7)

df['Survived'].value_counts()

#Encoding the ctagorical Data
from sklearn.preprocessing import LabelEncoder
lb = LabelEncoder()
df.iloc[:, 3] = lb.fit_transform(df.iloc[:, 3].values)




df.dtypes

fig = plt.figure(figsize = (12, 9))
plt.subplot2grid((2,3), (0,0))
df.Survived.value_counts().plot(kind='bar',alpha = 1)
plt.title('Survived')


plt.subplot2grid((2,3), (0,1))
plt.scatter(df.Survived, df.Age, alpha = 0.5, color = 'green')
plt.title('Survived')

plt.subplot2grid((2,3), (1,2))
df.Embarked.value_counts(normalize=True).plot(kind='bar',alpha = 1)
plt.title('Embarked/ Departure')

plt.subplot2grid((2,3), (1,0), colspan = 2)
for i in[1,2,3]:
  df.Age[df.Pclass == i].plot(kind = 'kde')
plt.title("age vs class")
plt.legend(("1st","2nd","3rd"))
plt.xlabel("Age:")
plt.ylabel("Class")

fig = plt.figure(figsize = (15, 11))
plt.subplot2grid((3,4), (0,0))
df.Survived.value_counts().plot(kind='bar',alpha = 1)
plt.title('Survived')

plt.subplot2grid((3,4), (0,1))
df.Survived[df.Sex == 1].value_counts().plot(kind='bar',alpha = 1)
plt.title('Men Survived')

plt.subplot2grid((3,4), (0, 2))
df.Survived[df.Sex == 0].value_counts().plot(kind='bar',alpha = 1, color ='pink')
plt.title(' Women Survived')

plt.subplot2grid((3,4), (0,3))
df.Sex[df.Survived == 1].value_counts(normalize = True).plot(kind='bar',alpha = 0.5, color = ['violet', 'blue'])
plt.title('Sex of Survived')

plt.subplot2grid((3,4), (2,0))
df.Survived[(df.Sex == 1) & (df.Pclass == 1)] .value_counts().plot(kind='bar',alpha = 1)
plt.title('Rich Men Survived')

plt.subplot2grid((3,4), (2,1))
df.Survived[(df.Sex == 1) & (df.Pclass == 3)] .value_counts().plot(kind='bar',alpha = 1)
plt.title('Poor Men Survived')

plt.subplot2grid((3,4), (2,2))
df.Survived[(df.Sex == 0) & (df.Pclass == 1)] .value_counts().plot(kind='bar',alpha = 1)
plt.title('Rich WoMen Survived')

plt.subplot2grid((3,4), (2,3))
df.Survived[(df.Sex == 0) & (df.Pclass == 3)] .value_counts().plot(kind='bar',alpha = 1)
plt.title('Poor WoMen Survived')



plt.subplot2grid((3,4), (1,0), colspan = 4)
for i in[1,2,3]:
  df.Survived[df.Pclass == i].plot(kind = 'kde')
plt.title("Class  vs survived")
plt.legend(("1st","2nd","3rd"))



plt.show()

# Women who were saved in percentage
train = pd.read_csv("train.csv")

train["Hyp"] = 0
train.loc[train.Sex == 0, "Hyp"] = 1

train["Result"] = 0
train.loc[train.Survived == train["Hyp"], "Result"] = 1

print(train["Result"].value_counts(normalize = True))

#Cleaning the data
def clean_data(data):
  data["Fare"] = data["Fare"].fillna(data["Fare"].dropna().median())
  date["Age"] = data["Age"].fillna(data["Age"].dropna().median())

  data["Embarked"] = data["Embarked"].fillna("S")  %for people who are unknown 
  data.loc[data["Embarked"] == "S", "Embarked"] = 0
  data.loc[data["Embarked"] == "C", "Embarked"] = 1
  data.loc[data["Embarked"] == "Q", "Embarked"] = 2

#Using Regression
import pandas as pd
import utils

train = pd.read_csv('train.csv')
utils.clean_data(train)

target = train["Survived"].values
feature_names = ["Pclass", "Age", "Sex", "SibSp", "Parch"]
features = train[[feature_names]].values

from sklearn.linear_model import LogisticRegression
log = LogisticRegression()
log_ = log.fit(target, features)

print log_.score(features, target)

poly = preprocessing.PolynomialFeatures(degree = 2)
ploy_features = poly.fit_transform(features)

log_ = log.fit(poly_features, target)
print log_.score(poly_features, target)

#Decision Tree 
from sklearn import tree , model_selection
train = pd.read_csv('train.csv')
utils.clean_data(train)

target = train["Survived"].values
feature_names = ["Pclass", "Age", "Sex", "SibSp", "Parch"]
features = train[[feature_names]].values

decision_tree = tree.DecisionTreeClassifier(random_state = 1)
decision_tree_ = decision_tree.fit(features, names)

print(decision_tree_.score(features, target))

scores = model_selection.cross_val_score(decision_tree, features, target, scoring = 'accuracy', cv = 50)
print(scores)
print(scores.mean())